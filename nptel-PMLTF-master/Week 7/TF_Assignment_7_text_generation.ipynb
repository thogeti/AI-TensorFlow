{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TF_Assignment-7 text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ada-nai/nptel-PMLTF/blob/master/Week%207/TF_Assignment_7_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c48e78da-1bb2-473f-8d47-2239a7f93a08"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD_55cOxLkAb",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavnuByVymwK",
        "colab": {}
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "colab": {}
      },
      "source": [
        "# Create mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "Use `tf.keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `tf.keras.layers.Embedding` (dim=256)\n",
        "* `tf.keras.layers.LSTM` (units=1024)\n",
        "* `tf.keras.layers.Dense`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "# Write the code here to define the model\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQuTy8bPT6u1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size = vocab_size, embedding_dim = embedding_dim, rnn_units = rnn_units, batch_size = BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d474f5ce-3c0b-4020-d1eb-72b3ac0cb410"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "a0a520b5-fc17-4620-e7bc-f4f89161d36b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "colab": {}
      },
      "source": [
        "# Write code here\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. We'll use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDl1_Een6rL0",
        "colab": {}
      },
      "source": [
        "# Write code here\n",
        "model.compile(optimizer='adam', loss=loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7yGBE2zxMMHs",
        "colab": {}
      },
      "source": [
        "EPOCHS=10\n",
        "fin = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "be447dea-d336-4945-839e-57abc43396c1"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 10s 61ms/step - loss: 2.6613\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 9s 51ms/step - loss: 1.9583\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.6911\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.5423\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 9s 51ms/step - loss: 1.4548\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 9s 51ms/step - loss: 1.3936\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.3489\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.3094\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.2747\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.2424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNZIRxksbAnC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9eeeb58-aa98-48ec-a862-2c8448b7cc89"
      },
      "source": [
        "history_1 = model.fit(dataset, epochs=fin, callbacks=[checkpoint_callback], initial_epoch= (EPOCHS))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 11/40\n",
            "172/172 [==============================] - 9s 51ms/step - loss: 1.2104\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 9s 49ms/step - loss: 1.1789\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 9s 51ms/step - loss: 1.1451\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.1112\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.0768\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 9s 51ms/step - loss: 1.0406\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.0061\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.9701\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.9343\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.9000\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.8677\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 9s 49ms/step - loss: 0.8379\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.8101\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 9s 49ms/step - loss: 0.7865\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.7646\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.7460\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.7292\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.7148\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.7010\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 9s 49ms/step - loss: 0.6913\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.6826\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 9s 49ms/step - loss: 0.6748\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.6671\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.6616\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.6556\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.6517\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.6469\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.6436\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 0.6402\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 8s 49ms/step - loss: 0.6399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### Restore the latest checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk2WJ2-XjkGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e57581f-dd38-4be2-e031-a14a65ab7abd"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_40'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71xa6jnYVrAN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "715f658a-5b9f-426b-9c49-ca54d3021d5c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### The prediction loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "e629d263-9832-45e0-a4f6-6eefa63d85ce"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: Know, when he's for you;\n",
            "Hop stery me to speak,\n",
            "And wounds all upon thy head\n",
            "And dull the watch is changedoubly one, and thy father York\n",
            "May them account their liberties. Let me be put to death;\n",
            "I'll pace a man born to dinner, I'll swear it.\n",
            "\n",
            "Shepherd:\n",
            "Come, bring her watch'd sound, to counsel throut my holy oath:\n",
            "Confess the foolish skining largues in uch laudy, one sungrone encounter men\n",
            "\n",
            "EDWARD:\n",
            "Lord Staft he did betray them.\n",
            "\n",
            "OXFORD:\n",
            "Farewell, farewell! one. Love with my nature, cousin, fear.\n",
            "\n",
            "Third Citizen:\n",
            "We are all undone! the harveham finger sounds,\n",
            "Bring forth her house, the very bled\n",
            "in their bed:\n",
            "\n",
            "ISABELLA:\n",
            "I am costent this debusy lord, I lose nothing\n",
            "When I shall show usage of you to any rage\n",
            "With flower throst letters to his part to tell,\n",
            "Signior Bagna, I must eatenced:\n",
            "That morth the garden rever me down to-morrow mocking now?\n",
            "\n",
            "DORSET:\n",
            "A bootless heart for be speed; what of\n",
            "France; or old weeps to cheer his songs\n",
            "therein throw an old trot seem your grace.\n",
            "\n",
            "First Murdere\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}